#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Utility files and modules"""

import os
import random
import time
from functools import lru_cache, wraps
from io import BytesIO
from itertools import combinations
from pathlib import Path
from typing import Optional, Union

import boto3
import librosa
import numpy as np
import pandas as pd
import requests
import sqlalchemy as sa
import torch
from dotenv import find_dotenv, load_dotenv
from loguru import logger
from pedalboard.io import AudioFile
from tqdm import tqdm


MODEL_URL = 'https://github.com/bleugreen/deeprhythm/raw/main/'

SEED = 42
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
EPS = 1e-4

SAMPLE_RATE = 22050    # used by default in `deeprhythm`
CLIP_LENGTH = 8
CLIP_SAMPLES = int(SAMPLE_RATE * CLIP_LENGTH)

AUDIO_CACHE_SIZE = 10240    # size of the cache we'll allow for AudioFile objects loaded from S3

# Setting these values ensures that we get 99.7% of beatstars tracks
MIN_BPM, MAX_BPM = 50, 200
MIN_DURATION, MAX_DURATION = 60, 360   # between 1 and 6 minutes

BPMS = np.array(range(MIN_BPM, MAX_BPM + 1))

# Gracefully load up environment variables whenever this module is imported
denv = find_dotenv()
if denv is not None:
    load_dotenv(denv)

# AWS resources
S3 = boto3.resource("s3")
S3_CLIENT = boto3.client("s3")
S3BucketClass = type(S3.Bucket('dummy'))
S3ObjectClass = type(S3.Object('dummy-bucket', 'dummy-key'))

EXTERNAL_BEATS_BUCKET = os.environ["EXTERNAL_BEATS_BUCKET"]
AUDIO_EXTS = ("wav", "mp3", "flac", "m4a", "mpeg")    # should be comprehensive


def get_weights(filename: str = "deeprhythm_colossal-0.7.pth", quiet: bool = False) -> str:
    # Construct the path to save the model weights
    home_dir = os.path.expanduser("~")
    model_dir = os.path.join(home_dir, ".local", "share", "deeprhythm_colossal")
    if not os.path.exists(model_dir):
        os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, filename)

    # Check if the model weights already exist
    if not os.path.isfile(model_path):
        logger.info("Downloading model weights...")
        # Download the model weights
        try:
            r = requests.get(MODEL_URL + filename, allow_redirects=True)
            if r.status_code == 200:
                with open(model_path, 'wb') as f:
                    f.write(r.content)
                logger.info("Model weights downloaded successfully.")
            else:
                logger.error(f"Failed to download model weights. HTTP Error: {r.status_code}")
        except Exception as e:
            logger.error(f"An error occurred during the download: {e}")
    else:
        if not quiet:
            logger.info("Model weights already exist.")

    return model_path


def split_audio(
        audio: np.ndarray,
        sr: int = SAMPLE_RATE,
        clip_length: int = CLIP_LENGTH,
) -> Optional[torch.Tensor]:
    """
    Split an audio array 8-second clips (padding where necessary), and return a single tensor of all clips.

    Parameters:
        - audio: Array generated by librosa.load representing the audio.
        - sr: Sampling rate to used for loading the audio.
        - clip_length: Length of each clip in seconds.

    Returns:
        A tensor of shape [clips, audio] where each row is an 8-second clip.
    """
    # Calculate the number of samples the clip lasts for
    clip_samples = sr * clip_length

    # Calculate how many clips we need, rounding up to cover the entire audio
    num_clips = (len(audio) + clip_samples - 1) // clip_samples

    # Pad audio if needed to make it evenly divisible by clip_samples
    padded_length = num_clips * clip_samples
    if padded_length > len(audio):
        pad_width = padded_length - len(audio)
        audio = np.pad(audio, (0, pad_width), mode='constant', constant_values=0)

    # Reshape into (num_clips, clip_samples)
    clips = audio.reshape(num_clips, clip_samples)

    # Convert to a torch tensor
    clip_tensor = torch.tensor(clips, dtype=torch.float32)

    # Raise an error if we have 0 as the first dim
    if clip_tensor.size(0) == 0:
        raise ValueError(f"Split tensor has no clips! Shape {clip_tensor.size()}!")

    return clip_tensor


def load_and_split_audio(
        filename: Union[str, Path],
        sr: int = SAMPLE_RATE,
        clip_length: int = CLIP_LENGTH,
) -> Optional[torch.Tensor]:
    """
    Load an audio file, split it into 8-second clips, and return a single tensor of all clips.

    Parameters:
        - filename: Path to the audio file.
        - sr: Sampling rate to use for loading the audio.
        - clip_length: Length of each clip in seconds.

    Returns:
        A tensor of shape [clips, audio] where each row is an 8-second clip.
    """

    audio, _ = librosa.load(filename, sr=sr, mono=True)
    return split_audio(audio, sr, clip_length)


def bpm_to_class(bpm: int, bpm_classes: np.ndarray = BPMS) -> int:
    """
    Return the index of the BPM value in the `bpm_classes` array.

    Raises:
         IndexError: if the BPM is not found in `bpm_classes`.
    """
    try:
        return int(np.where(bpm_classes == bpm)[0][0])
    except IndexError:
        raise IndexError(f"BPM value {bpm} not found in bpm_classes.")


def class_to_bpm(class_index: int, bpm_classes: np.ndarray = BPMS) -> int:
    """
    Map a class index back to a BPM value (to the center of the class interval).

    Raises:
        IndexError: if the class_idx is out of bounds for `bpm_classes`.
    """
    if class_index < 0:
        raise IndexError("Only positive indices supported")
    try:
        return int(bpm_classes[class_index])
    except IndexError:
        raise IndexError(f"Class index {class_index} out of bounds for BPM array with length {len(bpm_classes)}")


def lists_are_unique(*list_of_strings: list[str]) -> bool:
    """
    Takes in an arbitrary number of lists of strings and checks that they are all unique
    """
    for sp1, sp2 in combinations(list_of_strings, 2):
        if any(i in sp2 for i in sp1):
            return False
    return True


def timeit(func):
    """
    Wrapper function that times the execution of a function.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        elapsed = end - start
        # If the function returns multiple values in a tuple, keep that tuple but prepend the elapsed time
        if isinstance(result, tuple):
            return *result, elapsed
        else:
            return result, elapsed
    return wrapper


# @lru_cache(maxsize=1000)
def get_bucket(bucket_name: str) -> S3BucketClass:
    """
    Get a bucket from a string, with caching to prevent redundant reads
    """
    return S3.Bucket(bucket_name)


# @lru_cache(maxsize=1000)
def get_object(obj: str, bucket: Union[str, S3BucketClass], **kwargs) -> S3ObjectClass:
    """
    Grab a S3 object from a bucket
    """
    if isinstance(bucket, str):
        bucket = get_bucket(bucket)
    return S3_CLIENT.get_object(Key=obj, Bucket=bucket.name, **kwargs)


# @lru_cache(maxsize=None)
def get_db_engine(
        user: Optional[str] = None,
        password: Optional[str] = None,
        hostname: Optional[str] = None
) -> sa.engine.base.Engine:
    """
    Creates database engine in sqlalchemy, can be passed easily to e.g. pandas
    """
    # Use environment variables if not passed
    user_ = user if user is not None else os.environ.get("DB_USERNAME")
    password_ = password if password is not None else os.environ.get("DB_PASSWORD")
    hostname_ = hostname if hostname is not None else os.environ.get("DB_HOSTNAME")
    return sa.create_engine(
        f"postgresql+psycopg2://{user_}:{password_}@{hostname_}/postgres",
        pool_pre_ping=True,
    )


@lru_cache(maxsize=None)
def load_df_from_postgres(query: str) -> pd.DataFrame:
    """
    Loads up a dataframe with a given postgresql query
    """
    engine = get_db_engine()
    return pd.read_sql(query, engine)


def total_parameters(layer: torch.nn.Module) -> int:
    """Gets total number of parameters for a pytorch module"""
    return sum(p.numel() for p in layer.parameters())


def list_files_in_bucket(bucket: Union[str, S3BucketClass]) -> list[str]:
    """
    Returns all files in a bucket as a list of strings
    """
    if isinstance(bucket, str):
        bucket = get_bucket(bucket)
    return [f.key for f in tqdm(bucket.objects.all(), desc="Listing files in bucket...")]


# noinspection PyTypeChecker
@lru_cache(maxsize=AUDIO_CACHE_SIZE)
def read_s3_object_to_audiofile(
        obj: Union[str, S3ObjectClass],
        bucket: Union[str, S3BucketClass] = None
) -> AudioFile:
    """
    Read a S3 object and convert it to a pedalboard audio file
    """
    if bucket is None:
        bucket = get_bucket(EXTERNAL_BEATS_BUCKET)
    if isinstance(obj, str):
        obj = get_object(obj, bucket)
    obj_bytes = BytesIO(obj.get("Body").read())
    return AudioFile(obj_bytes)


def seed_everything(seed: int = SEED) -> None:
    """
    Set the random seeds for ML libraries.
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # safe to call even if cuda is not available
    random.seed(seed)
    np.random.seed(seed)


def get_project_root() -> Path:
    """
    Returns the root directory of the project.
    """
    # Possibly the root directory, but doesn't always work when running from the CLI for some reason
    poss_path = Path(__file__).parent.parent
    # The root directory should always have these files (this is pretty hacky)
    expected_files = ["tests", "pyproject.toml", "poetry.lock", ".gitignore"]
    if all((poss_path / fp).exists() for fp in expected_files):
        return poss_path
    else:
        return Path.cwd()
